// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
export const parser = LRParser.deserialize({
  version: 14,
  states: "%WQYQPOOO_QQO'#CtOsQQO'#CsOOQO'#Cl'#ClOOQO'#Cf'#CfQYQPOOO!OQPO'#CnOOQO,59X,59XO!WQPO'#ChO!]QQO,59`O!kQPO,59XOOQO'#Cu'#CuO!pQPO,59_OOQO-E6d-E6dO!uQPO'#CpOOQO'#Cg'#CgO!zQPO,59YOOQO,59Y,59YOOQO,59S,59SOOQO-E6f-E6fO#SQPO1G.sO#XQPO'#CtO#gQPO1G.yO#rQPO,59[OOQO-E6e-E6eOOQO1G.t1G.tOOQO7+$_7+$_O#zQPO,59`O$YQPO7+$eOOQO'#Cq'#CqOOQO1G.v1G.vOOQO<<HP<<HP",
  stateData: "$_~O_OSPOS~ORPO~OSYOUWOcUOVhXWhXXhX~OVZOWZOXZO~OR^OfaO~ORbO~OUWOVhaWhaXha~OTdO~OReO~OSgO~OR^OfiO~OcUO~OUWORhXShX]hX~OSlORgi]gi~ORmOTmO~OUWORhaSha]ha~OToO~O",
  goto: "!rjPPPPPPPPPPkqwPPP!R!V!ZP!a!eP!V!h!oQTOR]TQ`URh`QXPScXkRkeTSOTTROTQVPRjdT_U`RngSQOTRf[R[Q",
  nodeNames: "âš  LineComment start Ident : String && -> <- --",
  maxTerm: 25,
  skippedNodes: [0,1],
  repeatNodeCount: 3,
  tokenData: "&z~R`XY!TYZ!T]^!Tpq!Trs!fst$Svw$k}!O$v!Q![%u![!]&`!^!_&e!c!}%u#R#S%u#T#o%u#o#p&p#q#r&u~!YS_~XY!TYZ!T]^!Tpq!T~!iVOr!frs#Os#O!f#O#P#T#P;'S!f;'S;=`#|<%lO!f~#TOT~~#WRO;'S!f;'S;=`#a;=`O!f~#dWOr!frs#Os#O!f#O#P#T#P;'S!f;'S;=`#|;=`<%l!f<%lO!f~$PP;=`<%l!f~$XSP~OY$SZ;'S$S;'S;=`$e<%lO$S~$hP;=`<%l$S~$nPvw$q~$vOU~R${URP}!O%_!Q![%u!`!a&Z!c!}%u#R#S%u#T#o%uR%fTXQRP}!O%u!Q![%u!c!}%u#R#S%u#T#o%uP%zTRP}!O%u!Q![%u!c!}%u#R#S%u#T#o%uQ&`OVQ~&eOS~~&hP}!O&k~&pOW~~&uOc~~&zOf~",
  tokenizers: [0, 1],
  topRules: {"start":[0,2]},
  tokenPrec: 0
})
